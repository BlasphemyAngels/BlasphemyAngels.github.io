<blockquote>前面咱们介绍了`GAN`，但是`GAN`的训练是不稳定的，`DCGAN`通过将`GAN`和`CNN`结合，并在上面加入一些限制，使得`DCGAN`训练较为稳定，并且能够使用`DCGAN`训练得到好的图像特征表示，然后使用网络的生成器和判别器作为特征抽取用于有监督任务。</blockquote>

<!--more-->

<h3 id="模型">模型</h3>

<p>   文章的方法是基于最近在<code class="language-plaintext highlighter-rouge">CNN</code>架构上的三种变化提出的。</p>

<ul>
  <li><a href="https://arxiv.org/abs/1412.6806">Springenberg et al., 2014</a>提出的全卷积网络，使用大步长的卷积操作代替<code class="language-plaintext highlighter-rouge">采样</code>操作，这样使得网络学习它自己的空间采样。文章将这种方法用在了判别器和生成器中。</li>
  <li>消除卷积网络上端的全连接层。<a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">Mordvintsev et al.</a>在图像分类任务中使用全局平均采样代替全连接层，作者发现使用全局平均采用能够增加模型的稳定性而降低模型的收敛速度。因此文本直接将生成器和判别器的输入和输出的全连接层移除，而都采用卷积操作（卷积在一定程度上就是采样）。</li>
  <li>在除了生成器的输出层和判别器的输入层之外，每一层都使用<code class="language-plaintext highlighter-rouge">Batch Normalization(BN,批处理化)</code>。<code class="language-plaintext highlighter-rouge">BN</code>归一化每个神经元的输入为均值为0方差为1，这样解决了由低初始化引起的一些问题而且使得梯度在深度网络中传输的更好。<code class="language-plaintext highlighter-rouge">BN</code>已经被证明在<code class="language-plaintext highlighter-rouge">GAN</code>中很有用，使得<code class="language-plaintext highlighter-rouge">GAN</code>不会集中所有采样于一个点。</li>
  <li>对于生成器，除了输出层使用<code class="language-plaintext highlighter-rouge">tanh</code>激励函数以为其余层均使用<code class="language-plaintext highlighter-rouge">ReLU</code>。对于判别器，所有层都使用<code class="language-plaintext highlighter-rouge">LeakyReLU</code>。
   模型的总体结构如下：</li>
</ul>

<p><img src="https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/DCGAN_model.png?raw=true" alt="DCGAN_model" />
   上面是生成器，下面是判别器。</p>

<p>   生成器的模型其实就是卷积的逆操作，如下图：</p>

<p><img src="https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/DCGAN_dcnn.png?raw=true" alt="DCGAN_dcnn" /></p>

<h3 id="训练">训练</h3>

<h4 id="训练参数">训练参数</h4>
<ul>
  <li>训练图像使用<code class="language-plaintext highlighter-rouge">tanh</code>将数值约束到<code class="language-plaintext highlighter-rouge">[-1,1]</code></li>
  <li>batch size为128</li>
  <li>初始化使用均值为0，方差为0.02的高斯分布</li>
  <li><code class="language-plaintext highlighter-rouge">LeakyReLU</code>参数为0.2</li>
  <li>使用<code class="language-plaintext highlighter-rouge">Adam Optimizer</code>，学习率设置为0.0002，动量项\(\beta_1\)为0.5</li>
</ul>

<h4 id="实验">实验</h4>

<p>   关于实验这里就不多说了，详细看原论文。</p>

<p>   下图是模型在<code class="language-plaintext highlighter-rouge">LSUN</code>上面的效果。</p>

<h4 id="向量算数">向量算数</h4>

<p>   在<code class="language-plaintext highlighter-rouge">词嵌入</code>中，词向量<code class="language-plaintext highlighter-rouge">国王</code>-词向量<code class="language-plaintext highlighter-rouge">男人</code>+词向量<code class="language-plaintext highlighter-rouge">女人</code>的结果向量是一个跟词向量<code class="language-plaintext highlighter-rouge">女王</code>很相似的向量。</p>

<p>   作者发现对于生成器向量<code class="language-plaintext highlighter-rouge">z</code>也有这个特点，如下图:</p>

<p><img src="an with glasses, then sub" alt="DCGAN_va" /></p>

<h2 id="总结">总结</h2>

<p>   虽然<code class="language-plaintext highlighter-rouge">DCGAN</code>在原始<code class="language-plaintext highlighter-rouge">GAN</code>中加入了很多限制使得其训练变得稳定，但是实际上<code class="language-plaintext highlighter-rouge">DCGAN</code>的训练还是不太稳定。</p>
