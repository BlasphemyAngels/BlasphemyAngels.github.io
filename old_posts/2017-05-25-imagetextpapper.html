<p>   &lt;font color=#0099ff size=5&gt;Image-Text matching&lt;/font&gt;是与多个方向相关的一个任务，下面就展示一下我自己收集的一些论文。</p>

<!--more-->

<h2 id="正文">正文</h2>

<h3 id="2017">2017</h3>

<ul>
  <li>
    <p>Image Retrieval</p>

    <ul>
      <li>Under Review CVPR <font color="red">A类</font> 2017
        <ul>
          <li><a href="https://arxiv.org/abs/1511.06267">Asymmetrically Weighted CCA And Hierarchical Kernel Sentence Embedding For Image &amp; Text Retrieval<br /></a>引用量：1</li>
        </ul>
      </li>
      <li>in submission for TPAMI
        <ul>
          <li><a href="https://arxiv.org/abs/1704.03470">Learning Two-Branch Neural Networks for Image-Text Matching Tasks<br /></a>引用量：0</li>
        </ul>
      </li>
      <li>CVPR <font color="red">A类</font>
        <ul>
          <li><a href="https://arxiv.org/abs/1611.09392">Generating Holistic 3D Scene Abstractions for Text-based Image Retrieval<br /></a>引用量：0</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>VQA</p>

    <ul>
      <li>arXiv
        <ul>
          <li><a href="https://arxiv.org/abs/1701.07149">Hierarchical Recurrent Attention Network for Response Generation<br /></a>引用量：0</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Image Caption &amp; Image Annonation</p>

    <ul>
      <li>arXiv
        <ul>
          <li><a href="https://arxiv.org/abs/1705.05102y">Learning Semantics for Image Annotation<br /></a>引用量：0</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>RL</p>

    <ul>
      <li>
        <p>Under Review CVPR <font color="red">A类</font></p>

        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/1612.00563">Self-critical Sequence Training for Image Captioning<br /></a>引用量：3</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1612.00370">Improved Image Captioning via Policy Gradient optimization of SPIDEr<br /></a>引用量：4</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="2016">2016</h3>

<ul>
  <li>
    <p>基于检索</p>

    <ul>
      <li>
        <p>CVPR <font color="red">A类</font></p>

        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/1511.06078">Learning Deep Structure-Preserving Image-Text Embeddings<br /></a>引用量：19</p>
          </li>
          <li>
            <p><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Tran_Aggregating_Image_and_CVPR_2016_paper.pdf">Aggregating Image and Text Quantized Correlated Components<br /></a> 引用量：1</p>
          </li>
          <li>
            <p><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf">DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations<br /></a> 引用量：10</p>
          </li>
        </ul>
      </li>
      <li>
        <p>ECCV <font color="red">B类</font></p>

        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/1604.02426">CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples<br /></a>引用量：16</p>
          </li>
          <li>
            <p><a href="https://www.cise.ufl.edu/~zizhao/paper_list/eccv2016.pdf">Kernel-Based Supervised Discrete Hashing for Image Retrieval<br /></a>引用量：1</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1604.01325">Deep Image Retrieval: Learning global representations for image search<br /></a>引用量：12</p>
          </li>
        </ul>
      </li>
      <li>
        <p>arXiv</p>

        <ul>
          <li><a href="https://arxiv.org/abs/1608.07973">Linking Image and Text with 2-Way Nets<br /></a>引用量：1</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Multimodal</p>

    <ul>
      <li>
        <p>CVPR <font color="red">A类</font></p>

        <ul>
          <li><a href="http://z-yt.net/tmp/cvpr2016/content/papers/8851c601.p">MDL-CW: A Multimodal Deep Learning Framework with CrossWeights<br /></a> 引用量：0</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>图像生成</p>

    <ul>
      <li>
        <p>ICML <font color="red">A类</font></p>

        <ul>
          <li><a href="https://arxiv.org/abs/1605.05396">Generative Adversarial Text to Image Synthesis<br /></a>引用量：46</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Visual Grounding</p>

    <ul>
      <li>
        <p>ECCV <font color="red">B类</font></p>

        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/1608.00272">Modeling Context in Referring Expressions<br /></a>引用量：8</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1608.00525">Modeling Context Between Objects for Referring Expression Understanding<br /></a>引用量：3</p>
          </li>
          <li>
            <p><a href="https://link.springer.com/chapter/10.1007%2F978-3-319-46484-8_42">Structured Matching for Phrase Localization<br /></a> 引用量：2</p>
          </li>
        </ul>
      </li>
      <li>
        <p>CVPR <font color="red">A类</font></p>

        <ul>
          <li><a href="https://arxiv.org/abs/1511.07067">Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes<br /></a>引用量：6</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Image Caption &amp; Image Annonation</p>

    <ul>
      <li>
        <p>ICLR</p>

        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/1511.06361">Order-Embeddings of Images and Language<br /></a>引用量：31 <br />在视觉-语义的层级上对其做偏序结构处理</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1511.04590v1">Trainable performance upper bounds for image and video captioning<br /></a>引用量：2</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1511.02793">Generating Images from Captions with Attention<br /></a>引用量：25</p>
          </li>
        </ul>
      </li>
      <li>
        <p>CVPR <font color="red">A类</font></p>

        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/1506.01144">What value do explicit high level concepts have in vision to language problems?<br /></a>引用量：8</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1603.03925">Image Captioning with Semantic Attention<br /></a>引用量：36</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1605.05395">Learning Deep Representations of Fine-grained Visual Descriptions<br /></a>引用量：16</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1603.08486">Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation<br /></a>引用量：4</p>
          </li>
        </ul>
      </li>
      <li>
        <p>ECCV <font color="red">B类</font></p>

        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/1607.08822">SPICE: Semantic Propositional Image Caption Evaluation<br /></a>引用量：12</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1512.03958v1">RNN Fisher Vectors for Action Recognition and Image Annotation<br /></a>引用量：5</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1605.01379">Leveraging Visual Question Answering for Image-Caption Ranking<br /></a>引用量：4</p>
          </li>
        </ul>
      </li>
      <li>
        <p>BMVC <font color="red">C类</font></p>

        <ul>
          <li><a href="https://arxiv.org/abs/1511.04590">Oracle performance for visual captioning<br /></a>引用量：2</li>
        </ul>
      </li>
      <li>
        <p>Meeting of the Association for Computational Linguistics</p>

        <ul>
          <li><a href="https://arxiv.org/abs/1603.06059">Generating Natural Questions About an Image<br /></a>引用量：1</li>
        </ul>
      </li>
      <li>
        <p>arXiv</p>

        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/1606.04621v2">Image Caption Generation with Text-Conditional Semantic Attention<br /></a>引用量：1</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1611.01646">Boosting Image Captioning with Attributes<br /></a>引用量：4</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1612.01887">Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning<br /></a>引用量：3</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>文本相似</p>

    <ul>
      <li>
        <p>HICSS</p>

        <ul>
          <li><a href="http://people.cs.georgetown.edu/~clay/research/pubs/shields-hicss-2016.pdf">Text-Based Document Similarity Matching Using Sdtext<br /></a> 引用量：0</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Generate Model</p>

    <ul>
      <li>
        <p>arXiv</p>

        <ul>
          <li><a href="https://arxiv.org/abs/1602.07416">Learning to Generate with Memory<br /></a>引用量：5</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Multi-task learning</p>

    <ul>
      <li>
        <p>SigKdd <font color="red">A类</font></p>

        <ul>
          <li><a href="http://www.kdd.org/kdd2016/papers/files/rpp0423-linA.pdf">Multi-Task Feature Interaction Learning<br /></a>引用量：0</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>VQA</p>

    <ul>
      <li>
        <p>ECCV <font color="red">B类</font></p>

        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/1511.05234">Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering<br /></a>引用量：39</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1606.08390">Revisiting Visual Question Answering Baselines<br /></a>引用量：14</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1604.04808">Learning Models for Actions and Person-Object Interactions with Transfer to Question Answering</a></p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1605.01379">Leveraging Visual Question Answering for Image-Caption Ranking<br /></a>引用量：4</p>
          </li>
        </ul>
      </li>
      <li>
        <p>CVPR <font color="red">A类</font></p>

        <ul>
          <li>
            <p><a href="http://z-yt.net/tmp/cvpr2016/content/papers/8851c064.pdf">Deep Supervised Hashing for Fast Image Retrieval<br /></a> 引用量：5</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1511.03416">Visual7W: Grounded Question Answering in Images<br /></a>引用量：34 <br /> 李飞飞老师的文章，这篇提出了一个新的数据集Visual7W</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1511.07394">Where To Look: Focus Regions for Visual Question Answering<br /></a>引用量：29 <br />加入attention机制的一篇文章</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1511.06973">Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources<br /></a>引用量：12 <br /> 沈春华老师的文章，这篇加入了外接知识库</p>
          </li>
          <li>
            <p><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Kafle_Answer-Type_Prediction_for_CVPR_2016_paper.pdf">Answer-Type Prediction for Visual Question Answering<br /></a> 引用量：7</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1511.05756">Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction<br /></a></p>
          </li>
          <li>
            <p><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Stacked_Attention_Networks_CVPR_2016_paper.pdf">Stacked Attention Networks for Image QuestionAnswering<br /></a> 采用多次关注聚焦的方式来处理定位问题关注点</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1511.02799">Neural Module Networks</a> <br />根据问题不同动态组合网络</p>
          </li>
          <li>
            <p><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Tapaswi_MovieQA_Understanding_Stories_CVPR_2016_paper.pdf">MovieQA: Understanding Stories in Movies through Question-Answering</a></p>
          </li>
          <li>
            <p><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhang_Yin_and_Yang_CVPR_2016_paper.pdf">Yin and Yang: Balancing and Answering Binary Visual Questions</a></p>
          </li>
        </ul>
      </li>
      <li>
        <p>arXiv</p>

        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/1610.01465">Visual Question Answering: Datasets, Algorithms, and Future Challenges<br /></a> 综述性文章</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1607.05910">Visual Question Answering: A Survey of Methods and Datasets<br /></a> 综述性文章</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1603.02814">Image Captioning and Visual Question Answering Based on Attributes and External Knowledge<br /></a> 沈春华老师的文章，提取高层次语义概念的图像特征</p>
          </li>
        </ul>
      </li>
      <li>
        <p>AAAI <font color="red">A类</font></p>

        <ul>
          <li><a href="https://arxiv.org/abs/1506.00333">Learning to Answer Questions From Image Using Convolutional Neural Network<br /></a></li>
        </ul>
      </li>
      <li>
        <p>NIPS <font color="red">A类</font></p>

        <ul>
          <li><a href="https://arxiv.org/abs/1606.00061">Hierarchical Question-Image Co-Attention for Visual Question Answering<br /></a> 采用图像attention问题，再用问题attention图像</li>
        </ul>
      </li>
      <li>
        <p>PMLR</p>

        <ul>
          <li><a href="https://arxiv.org/abs/1603.01417">Dynamic Memory Networks for Visual and Textual Question Answering<br /></a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>图像匹配</p>

    <ul>
      <li>
        <p>ECCV <font color="red">B类</font></p>

        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/1605.05923v1">Matching Handwritten Document Images<br /></a>引用量：3</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1603.06041v1">Learning Image Matching by Simply Watching Video<br /></a>引用量：3</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Cross-Modal Retrieval</p>

    <ul>
      <li>
        <p>SigKdd <font color="red">A类</font></p>

        <ul>
          <li><a href="http://www.kdd.org/kdd2016/papers/files/rpp0086-caoA.pdf">Deep Visual-Semantic Hashing for Cross-Modal Retrieval<br /></a>引用量：6</li>
        </ul>
      </li>
      <li>
        <p>ICMR <font color="red">B类</font></p>

        <ul>
          <li><a href="http://dl.acm.org/citation.cfm?doid=2911996.2912000">Correlation Autoencoder Hashing for Supervised Cross-Modal Search<br /></a>引用量：2</li>
        </ul>
      </li>
      <li>
        <p>arXiv</p>

        <ul>
          <li><a href="https://arxiv.org/abs/1602.06697">Correlation Hashing Network for Efficient Cross-Modal Retrieval<br /></a>引用量：3</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>RL</p>

    <ul>
      <li>
        <p>ICLR</p>

        <ul>
          <li><a href="https://arxiv.org/abs/1511.06732">Sequence Level Training with Recurrent Neural Networks<br /></a>引用量：56</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="2015">2015</h3>

<ul>
  <li>
    <p>基于检索</p>

    <ul>
      <li>
        <p>ICCV <font color="red">A类</font></p>

        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/1504.06063">Multimodal Convolutional Neural Networks for Matching Image and Sentence<br /></a>引用量：31</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1509.06243">LEWIS: Latent Embeddings for Word Images and their Semantics<br /></a>引用量：1</p>
          </li>
        </ul>
      </li>
      <li>
        <p>CVPR <font color="red">A类</font></p>

        <ul>
          <li>
            <p><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Klein_Associating_Neural_Word_2015_CVPR_paper.pdf">Associating Neural Word Embeddings with Deep Image Representations using Fisher Vectors<br /></a> 引用量：21</p>
          </li>
          <li>
            <p><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_012_ext.pdf">Deep correlation for matching images and text<br /></a> 引用量：31 <br />采用 canonical correlation analysis 作为目标函数，但这种目标函数不容易求导</p>
          </li>
        </ul>
      </li>
      <li>
        <p>arXiv</p>

        <ul>
          <li><a href="https://arxiv.org/abs/1601.03478">Deep Learning Applied to Image and Text Matching<br /></a>引用量：0 <br />类似综述类的一类文章，文章很长，但是在后半段作者阐述了自己的工作。总的来说，前面的综述部分还可以。</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>基于生成</p>

    <ul>
      <li>
        <p>ICLR</p>

        <ul>
          <li><a href="https://arxiv.org/abs/1412.6632v5">Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)<br /></a>引用量：181</li>
        </ul>
      </li>
      <li>
        <p>CVPR <font color="red">A类</font></p>

        <ul>
          <li><a href="https://arxiv.org/abs/1411.4555">Show and Tell: A Neural Image Caption Generator<br /></a>作者：Bengio 引用量：583</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Image Matching</p>

    <ul>
      <li>
        <p>ICCV <font color="red">A类</font></p>

        <ul>
          <li><a href="https://arxiv.org/abs/1505.04845">Multi-Image Matching via Fast Alternating Minimization<br /></a>引用量：7</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Image Caption &amp; Image Annonation</p>

    <ul>
      <li>
        <p>CVPR <font color="red">A类</font></p>

        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/1412.2306">Deep Visual-Semantic Alignments for Generating Image Descriptions<br /></a>引用量：535</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1411.5654">Learning a Recurrent Visual Representation for Image Caption Generation<br /></a>引用量：76</p>
          </li>
        </ul>
      </li>
      <li>
        <p>ICML <font color="red">A类</font></p>

        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/1502.03044">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention<br /></a>引用量：537</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1502.03671">Phrase-based Image Captioning<br /></a>引用量：26</p>
          </li>
        </ul>
      </li>
      <li>
        <p>ICCV <font color="red">A类</font></p>

        <ul>
          <li><a href="https://arxiv.org/abs/1509.04942">Guiding Long-Short Term Memory for Image Caption Generation<br /></a>引用量：1</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>图像语义分割</p>

    <ul>
      <li>
        <p>CVPR <font color="red">A类</font></p>

        <ul>
          <li><a href="https://arxiv.org/abs/1411.4038">Fully Convolutional Networks for Semantic Segmentation<br /></a>引用量：902</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Multi-task learning</p>

    <ul>
      <li>
        <p>CVPR <font color="red">A类</font></p>

        <ul>
          <li><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Pentina_Curriculum_Learning_of_2015_CVPR_paper.pdf">Curriculum Learning of Multiple Tasks</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>VQA</p>

    <ul>
      <li>
        <p>ICCV <font color="red">A类</font></p>

        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/1505.01121">Ask Your Neurons: A Neural-based Approach to Answering Questions about Images<br /></a></p>
          </li>
          <li>
            <p><a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf">VQA: Visual Question Answering<br /></a>
  提出了目前最大的数据集mscocoQA 网页：http://www.visualqa.org/</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1506.00278">Visual Madlibs: Fill in the blank Image Generation and Question Answering</a></p>
          </li>
        </ul>
      </li>
      <li>
        <p>NIPS <font color="red">A类</font></p>

        <ul>
          <li><a href="https://arxiv.org/abs/1505.02074">Exploring Models and Data for Image Question Answering<br /></a></li>
        </ul>
      </li>
      <li>
        <p>CVPR <font color="red">A类</font></p>

        <ul>
          <li>
            <p><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Lin_Dont_Just_Listen_2015_CVPR_paper.pdf">Don’t Just Listen, Use Your Imagination: Leveraging Visual Common Sense for Non-Visual Tasks</a></p>
          </li>
          <li>
            <p><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Sadeghi_VisKE_Visual_Knowledge_2015_CVPR_paper.pdf">VisKE: Visual Knowledge Extraction and Question Answering by Visual Verification of Relation Phrases</a></p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="2014">2014</h3>

<ul>
  <li>
    <p>基于检索</p>

    <ul>
      <li>
        <p>NIPS <font color="red">A类</font></p>

        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/1406.5679">Deep Fragment Embeddings for Bidirectional Image Sentence Mapping<br /></a>引用量:161</p>
          </li>
          <li>
            <p><a href="https://arxiv.org/abs/1411.2539">Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models<br /></a>引用量：202</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>基于生成</p>

    <ul>
      <li>
        <p>CVPR <font color="red">A类</font></p>

        <ul>
          <li><a href="https://arxiv.org/abs/1411.4952v3">From Captions to Visual Concepts and Back<br /></a>引用量:180</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Visual Grounding</p>
  </li>
  <li>
    <p>Multimodal Representations</p>

    <ul>
      <li>
        <p>TPAMI <font color="red">A类</font></p>

        <ul>
          <li><a href="https://core.ac.uk/download/pdf/19967857.pdf">Spherical and Hyperbolic Embeddings of Data<br /></a></li>
        </ul>
      </li>
      <li>
        <p>ICML</p>

        <ul>
          <li><a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2014c2_kiros14.pdf">Multimodal neural language models</a>
  log-bilinear nerual language models</li>
        </ul>
      </li>
      <li>
        <p>arXiv</p>

        <ul>
          <li><a href="http://www.cs.cmu.edu/~ark/EMNLP-2015/proceedings/VL/pdf/VL04.pdf">Explain images with multi-modal recurrent neural networks</a>
  RNN</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Image Caption &amp; Image Annonation</p>

    <ul>
      <li>
        <p>Meeting of the Association for Computational Linguistics</p>

        <ul>
          <li><a href="http://acl2014.org/acl2014/P14-2/xml/P14-2074.xhtml">Comparing Automatic Evaluation Measures for Image Description<br /></a>引用量：42 论文比较了几种常见的图像描述问题的评价方法</li>
        </ul>
      </li>
      <li>
        <p>Computer Science</p>

        <ul>
          <li><a href="https://arxiv.org/abs/1410.1090?context=cs.CV">Explain Images with Multimodal Recurrent Neural Networks<br /></a>引用量：94</li>
        </ul>
      </li>
      <li>
        <p>ICLR</p>

        <ul>
          <li><a href="https://arxiv.org/abs/1312.4894">Deep Convolutional Ranking for Multilabel Image Annotation<br /></a>引用量：71</li>
        </ul>
      </li>
      <li>
        <p>ECCV</p>

        <ul>
          <li><a href="http://slazebni.cs.illinois.edu/publications/yunchao_eccv14_sentence.pdf">Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections</a>
  normalized CCA</li>
        </ul>
      </li>
      <li>
        <p>TACL</p>

        <ul>
          <li><a href="http://www.cl.uni-heidelberg.de/courses/ws14/deepl/SocherETAL14.pdf">Grounded compositional semantics for findingand describing images with sentences</a>
  dependency tree recursive networks</li>
        </ul>
      </li>
      <li>
        <p>ACL 2014 Workshop on Semantic Parsing</p>

        <ul>
          <li><a href="">A deeparchitecture for semantic parsing</a>
  proposed a two-step embedding and generation procedure for semantic parsing.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>VQA</p>

    <ul>
      <li>
        <p>NIPS <font color="red">A类</font></p>

        <ul>
          <li><a href="https://arxiv.org/abs/1410.0210v2">A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input<br /></a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>图像语义分割</p>

    <ul>
      <li>
        <p>CVPR <font color="red">A类</font></p>

        <ul>
          <li><a href="https://arxiv.org/abs/1311.2524">Rich feature hierarchies for accurate object detection and semantic segmentation Tech report (v5)<br /></a>引用量：1969</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="2013-or-before">2013 or before</h3>

<ul>
  <li>
    <p>Image Retrieval</p>

    <ul>
      <li>
        <p>TPAMI <font color="red">A类</font></p>

        <ul>
          <li>2007–<a href="https://www.cs.swarthmore.edu/~turnbull/cs97/f09/paper/pami07-semantics.pdf">Supervised Learning of Semantic Classes for Image Annotation and Retrieval<br /></a> 引用量：980</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Multimodal Learning</p>

    <ul>
      <li>
        <p>NIPS <font color="red">A类</font></p>

        <ul>
          <li>2012–<a href="http://datascienceassn.org/sites/default/files/Multimodal%20Learning%20with%20Deep%20Boltzmann%20Machines.pdf">Multimodal Learning with Deep Boltzmann Machines<br /></a> 引用量：362</li>
        </ul>
      </li>
      <li>
        <p>ICML</p>

        <ul>
          <li><a href="http://ai.stanford.edu/~ang/papers/nipsdlufl10-MultimodalDeepLearning.pdf">Mul-timodal deep learning</a>
  autoencoders</li>
        </ul>
      </li>
      <li>
        <p>ICCV</p>

        <ul>
          <li>2011–<a href="http://people.eecs.berkeley.edu/~trevor/iccv11-mm.pdf">Learning cross-modality similarity formultinomial data</a>
  topic models</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="kernel-cca-normalized-cca">kernel CCA normalized CCA</h3>
