<blockquote>今天来分享EM算法，几个月前，通过阅读各种文献终于对EM算法有了较为清晰的认识，一直就想写篇博客总结一些，可惜拖延症拖延至今(QAQ)。期望最大算法（EM算法）是一种从不完全数据或有数据丢失的数据集（存在隐含变量）中求解概率模型参数的最大似然估计方法。先大体知道这么回事，理解了整篇文章再回来品读这句话。</blockquote>
<!--more-->
<h2 id="正文">正文</h2>
<h3 id="必备知识">必备知识</h3>
<h4 id="jenson不等式">jenson不等式</h4>
<p>   jenson不等式以丹麦数学家(Johan Jensen)命名。它给出积分的凸函数值和凸函数的积分值间的关系。jenson不等式的具体内容是啥呢？
   若f(x)在(a,b)之间是凸函数，那么：
\(f(\sum\_{i=1}^k\theta\_i x\_i)\leq\sum\_{i=1}^k\theta\_i f(x\_i)\)
   其中\(\theta_1,\theta_2,…,\theta_k\geq0,\sum_{i=1}^k\theta_i=1\)
   对于连续函数：
   若 \(p(x)&gt;0 \quad on \quad S\subseteq dom f, \int_Sp(x)dx=1\)
   则：
\(f(\int\_Sp(x)dx)\leq\int\_Sf(x)p(x)dx\)
   Jensen不等式是关于凸性(convexity)的不等式。凸性是一个很好的性质，在最优化问题里面，线性和非线性不是本质的区别，只有凸性才是。如果最优化的函数是凸的，那么局部最优就意味着全局最优，否则无法推得全局最优。有很多不等式都可以用Jensen不等式证得，从而可以把他们的本质归结为凸性。例如，均值不等式：
\(\frac{\sum\_{i=1}^nx\_i}{n}\geq(\prod\_{i=1}^nx\_i)^{\frac{1}{n}}\)
   本质上可以归结为对数函数log(x)的凸性。</p>
<h4 id="聚类">聚类</h4>
<p>   首先让我们回忆一下<code class="language-plaintext highlighter-rouge">k-means</code>算法。
   <code class="language-plaintext highlighter-rouge">k-means</code>算法，也叫k-平均或者k-均值算法，是一种广泛使用的聚类算法，其经常作为很多聚类算法的基础。
   假设输入样本为 \(S=x_1,x_2,…,x_m\)
   那么算法步骤为：</p>
<ul>
  <li>选择初始的k个簇中心: \(\mu_1,\mu_2,…\mu_k\)</li>
  <li>将每个样本x标记为距离簇中心最近的簇：
\(label\_i=argmin\_{1\leq j\leq i}||x\_i-u\_j||\)</li>
  <li>更新簇中心：
\(\mu\_j=\frac{1}{|c\_j|}\sum\_{i\in c\_j}x\_i\)</li>
  <li>重复最后两个步骤，指导满足终止条件。
   终止条件有很多，可以使用<code class="language-plaintext highlighter-rouge">迭代次数</code>，<code class="language-plaintext highlighter-rouge">簇中心变化率</code>，<code class="language-plaintext highlighter-rouge">最小平方误差MSE</code>。
   现在来思考一下，经典的<code class="language-plaintext highlighter-rouge">k-means</code>算法可以很方便的将未标记的样本分成多个集合，但并不能给出一个样本属于某一个集合的后验概率，当然咱们可以使用一些类似SVM中的方式得到类似于概率的数值。
    <h4 id="最大似然估计">最大似然估计</h4>
    <p>   最大似然估计属于统计学的内容，其常常用来估计样本的分布，找出与样本分布最接近的概率分布模型，它是一种非参数估计。其简单有效的估计方式使得其在应用中占据重要地位。
   假如小红有一枚硬币，她抛了十次，结果分别是，<code class="language-plaintext highlighter-rouge">正反反正反反反反正反</code>
   假设p为每次抛硬币结果为正的概率，则出现上面结果的概率为：
\(\begin{align}
P&amp;=p(1-p)(1-p)p(1-p)(1-p)(1-p)(1-p)p(1-p)\\\\
&amp;=p^3(1-p)^7
\end{align}\)
   重点来了，最大似然概率的思想就是最大化似然概率，什么是似然概率，似然概率就是出现当前样本的概率，也就是上面的概率P，那么怎么最大化上面的概率P呢？小时候学的微积分派上用场了：一阶导为0求极点…。可以求得上式的最优解为:<code class="language-plaintext highlighter-rouge">p=0.3</code>
   上面的整个思路历程就是最大似然估计的全过程。
   将上面的例子更泛化一点，投N次硬币，得到结果，其中n次朝上，N-n次朝下。
   假定朝上概率为p，则似然函数为：
\(P=p^n(1-p)^{N-n}\)
   因为对数函数是严格单增，为了方便计算，通常将似然函数变为对数似然函数计算：
\(\begin{align}
P&amp;=log(p^n(1-p)^{N-n})\\\\
&amp;=nlog(p)+(N-n)log(1-p)
\end{align}\)
求最大：
\(\frac{n}{p}-\frac{N-n}{1-p}=0 \Rightarrow p=\frac{n}{N}\)
   更进一步的，给定一组样本\(x_1,x_2,…,x_n\)，已知他们来自于高斯分布\(N(\mu,\sigma)\)，试估计\(\mu,\sigma\)
   写出似然函数：
   已知高斯分布的概率密度函数为：
\(f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x\_i-\mu)^2}{2\sigma^2}}\)
   则似然函数为：
\(L(x)=\prod\_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\)
   对数似然：
\(\begin{align}
l(x)&amp;=log(L(x))=\sum\_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\\\\
&amp;=\left( \sum\_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma} \right) + \left( \sum\_{i=1}^ne^{-\frac{(x-\mu)^2}{2\sigma^2}} \right)\\\\
&amp;=-\frac{n}{2}log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum\_{i=1}^n(x\\_i-\mu)^2
\end{align}\)</p>
  </li>
</ul>

<p>   最大化似然函数，分别对\(\mu和\sigma\)求偏导求极值得：</p>

<p>\(\begin{cases}
\mu=\frac{1}{n}\sum_{i}x\_i\\\\
\sigma^2=\frac{1}{n}\sum_{i}(x\_i-\mu)^2
\end{cases}\)
   可以发现咱们使用最大似然估计得到\(\mu和\sigma\)的值很符合直观想象：样本的均值就是高斯分布的均值，样本的方差就是高斯分布的方差。</p>
<h3 id="提出问题">提出问题</h3>
<p>   然而我们遇到的问题的随机变量有的时候是无法直接观察到的。比如：有N个人，其中有男性有女性，他们的身高分别服从\(N(\mu_1,\sigma_1)\)和\(N(\mu_2,\sigma_2)\)分布，要估计\(\mu_1，\sigma_1，\mu_2，\sigma_2\)。
   我们如果知道哪些是男性哪些是女性就好了，只需要分别对他们使用上面的最大似然估计。但是遗憾的是我们并不知道，因此我们需要使用EM算法。想一想这个问题，我们也可以先对数据进行聚类，得到男性和女性两类，再分别建模。待我们学完EM算法，我们会发现其实两种方法的思想是相似的。最后我们会分析一下k-means和EM算法的关系。
   上面的问题包含两个高斯模型，因此这类问题又称为高斯混合模型（GMM），高斯混合模型是EM算法解决问题的一个基本问题。</p>
<h3 id="gmm的直观解法">GMM的直观解法</h3>
<p>   随机变量X是由K个高斯分布混合而成，取各个高斯分布的概率为\(\pi_1,\pi_2,…,\pi_K\)，第i个高斯分布的均值为\(\mu_i\)，方差为\(\sum_i\)。随机变量X的一组样本为\(x_1,x_2,…,x_n\)，估计参数\(\pi，\mu，\sum\)。
   按上面的步骤计算，先写出似然函数：</p>

\[L(x)=\prod\_{i=1}^n\sum\_{k=1}^K\pi\_kN(x\_i|\mu\_i,\sum\_k)\]

<p>   取对数：
\(l(x)=\sum\_{i=1}^n log \left( \sum\_{k=1}^K\pi\_k N(x\_i|\mu\_i,\sum\_k) \right)\)
   显然上面的式子无法直接用求导的方式找最大值，为了解决这个问题，我们将问题分为两步：</p>
<ul>
  <li>估计数据来自哪一部分
   对于每个样本\(x_i\)，它由第k个高斯分布生成的概率为：
\(\gamma(i,k)=\frac{\pi\_kN(x\_i|\mu\_k,\sum\_k)}{\sum\_{j=1}^K \pi\_jN(x\_i|\mu\_j,\Sigma\_j)}\)
   注意上面的式子中的\(\mu和\sigma\)也是待估计的值，因此采用采样迭代法：首先先验的给出\(\mu和\sigma\)，然后计算\(\gamma\)，再通过计算出的\(\gamma\)计算\(\mu和\sigma\)，重复达到收敛即可。
   这里的\(\gamma(i,k)\)也可以看做第k个高斯分布在生成数据\(x_i\)时所做的贡献。
   上面已经粗略的说了计算的步骤，那么\(\mu和\sigma\)怎么由\(\gamma\)计算呢?</li>
  <li>估计每个高斯分布的参数
   对于每个高斯分布\(N_k\)而言，可以看做由它生成了\({\gamma(i,k)x_i|i=1,2,…,n}\)这些点。由于是高斯分布，所以直接使用前面讨论过的结果即可：
\(\begin{cases}
\mu=\frac{1}{n}\sum_{i}x\_i\\\\
\sigma^2=\frac{1}{n}\sum_{i}(x\_i-\mu)^2
\end{cases}\)
   得到：
\(\begin{cases}
N\_k=\sum\_{i=1}^N\gamma(i,k)\\\\
\mu_k=\frac{1}{N\_k}\sum\_{i=1}^{N\_k}\gamma(i,k)x\_i\\\\
\sum\_k=\frac{1}{N\_k}\sum\_{i=1}^{M\_k}\gamma(i,k)(x\_i-\mu\_k)(x\_i-\mu\_k)^T\\\\
\pi\_k=\frac{N\_k}{N}=\frac{1}{N}\sum\_{i=1}^{N}\gamma(i,k)
\end{cases}\)
   重复上面的两步，达到收敛条件（各参数无明显变化等）即可。这就是EM算法的直观过程。
    <h3 id="em算法">EM算法</h3>
    <p>   假设有数据集\({x_1,x_2,…,x_n}\)包含m个独立样本，希望从中找到该组数据的模型\(p(x,z)\)的参数。
   写出对数似然：
\(\begin{align}
l(\theta)&amp;=\sum_{i=1}^nlog(p(x\_i, \theta))\\\\
&amp;=\sum_{i=1}^nlog(\sum_z p(x,z;\theta))（全概率公式）
\end{align}\)
   z是隐随机变量，不方便直接找到参数估计。因此使用一个求最大值时常用的一个策略：计算\(l(\theta)\)的下界，求这个下界的最大值；重复这个过程，直到收敛到局部最大值。先求到的下界有可能比较宽松，不断重复，使得精度提升，使得求到的下界更紧，从而最终接近真实结果。
<img src="https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/em_tuidao.png?raw=true" alt="em_tuidao" />
   令\(Q_i\)是z的某一个分布，\(Q_i\geq0\)，使用jenson不等式：
\(\begin{align}
l(\theta)&amp;=\sum\_{i=1}^n log\sum\_z p(x,z;\theta)=\sum\_{i=1}^nlog\sum\_{z\_i}p(x\_i,z\_i;\theta)\\\\
&amp;=\sum\_{i=1}^nlog\sum\_{z\_i}Q\_i(z\_i)\frac{p(x\_i,z\_i;\theta)}{Q\_i(z\_i)}\\\\
&amp;\geq\sum\_{i=1}^n\sum\_{z\_i}Q\_i(z\_i)log\frac{p(x\_i,z\_i;\theta)}{Q\_i(z\_i)}
\end{align}\)
<img src="https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/em_jenson.png?raw=true" alt="em_jenson" />
   为了取等号，使得：
\(\frac{p(x\_i,z\_i;\theta)}{Q\_i(z\_i)}=c\)
   由此：
\(Q\_i(z\_i)\propto p(x\_i,z\_i;\theta) 又\sum\_zQ\_i(z\_i)=1\)
\(\begin{align}
\Rightarrow Q\_i(z\_i)&amp;=\frac{p(x\_i,z\_i;\theta)}{\sum\_zp(x\_i,z\_i;\theta)}\\\\
&amp;=\frac{p(x\_i,z\_i;\theta)}{p(x\_i;\theta)}\\\\
&amp;=p(z\_i;x\_i,\theta)
\end{align}\)
   如此我们就得到EM算法的整体框架：</p>
  </li>
  <li>E-step 对于每个i，令\(Q_i(z_i)=p(z_i;x_i,\theta)\)</li>
  <li>M-step 求\(\theta=argmax_{\theta}\sum_i\sum_{z_i}Q_i(z_i)log\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}\)
   重复进行上述两个步骤直至收敛。
   至于EM算法的收敛性证明，我就不说了，直接上从网上找的一段证明：
<img src="https://github.com/BlasphemyAngels/MarkDownPhotos/blob/master/em_proof.png?raw=true" alt="em_proof" />
    <h3 id="使用em算法解决gmm问题">使用EM算法解决GMM问题</h3>
    <p>   还记得咱们刚开始推到的GMM问题的解决方法吗？咱们当时是通过计算每一个高斯分布在每一个数据中占的比例估计每一个高斯分布的样本数据，然后再估计高斯分布的参数。现在咱们使用上面推导的<code class="language-plaintext highlighter-rouge">EM</code>算法解决一些这个问题。
   随机变量X是由K个高斯分布混合而成，取各个高斯分布的概率为\(\varphi_1,\varphi_2,…,\varphi_K\)，第i个高斯分布的均值为\(\,u_i\)，方差为\(\sum_i\)。若观测到随机变量X的一系列样本\(x_1,x_2,…,x_n\)，试估计参数\(\phi,\mu,\sum\)。</p>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>E-step \(Q_i(z_i=j)=P(z_i=j</td>
          <td>x_i;\phi,\mu,\sum)\)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>M-step 将多项式分布和高斯分布的参数带入：
\(\begin{align}
\sum\_{i=1}^n\sum\_{z\_i}&amp;Q\_i(z\_i)log\frac{p(x\_i,z\_i;\phi,\mu,\sum)}{Q\_i(z\_i)}\\\\
&amp;=\sum\_{i=1}^n\sum\_{j=1}^KQ\_i(z\_i=j)log\frac{p(x\_i|z\_i;\phi,\mu,\sum)p(z\_i=j;\phi)}{Q\_i(z\_i=j)}\\\\
&amp;=\sum\_{i=1}^n\sum\_{j=1}^KQ\_i(z\_i=j)log\frac{\frac{1}{(2\pi)^{n/2}|\Sigma\_j|^{1/2}}exp(-\frac{1}{2}(x\_i-\mu_j)^T\Sigma\_j^{-1}(x\_i-\mu_j))\phi\_j}{Q\_i(z\_i=j)}
\end{align}\)
   对均值求导：
\(\begin{align}
\bigtriangledown\_{\mu\_l}&amp;\sum\_{i=1}^n\sum\_{j=1}^KQ\_i(z\_i=j)log\frac{\frac{1}{(2\pi)^{n/2}|\Sigma\_j|^{1/2}}exp(-\frac{1}{2}(x\_i-\mu_j)^T\Sigma\_j^{-1}(x\_i-\mu_j))\phi\_j}{Q\_i(z\_i=j)}\\\\
&amp;=-\bigtriangledown\_{\mu\_l}\sum\_{i=1}^n\sum\_{j=1}^KQ\_i(z\_i=j)\frac{1}{2}(x\_i-\mu\_j)\Sigma\_j^{-1}(x\_i-\mu\_j)\\\\
&amp;=-\frac{1}{2}\sum\_{i=1}^n\sum\_{j=1}^KQ\_i(z\_i=j)\bigtriangledown\_{\mu\_l}(x\_i^T\Sigma\_l^{-1}x\_i-x\_i^T\Sigma\_j^{-1}\mu_j-\mu^T\Sigma\_j^{-1}x\_i+\mu^T\Sigma\_j^{-1}\mu\_j)\\\\
&amp;=-\frac{1}{2}\sum\_{i=1}^nQ\_i(z\_i=l)(-x\_i^T\Sigma\_l^{-1}-x\_i^T\Sigma\_l^{-1}+2\mu\_l^T\Sigma\_l^{-1})\\\\
&amp;=\sum\_{i=1}^nQ\_i(z\_i=l)(x\_i^T\Sigma\_l^{-1}-\mu\_l^T\Sigma\_l^{-1})
\end{align}\)
   令等式为0，得到均值为：
\(\mu\_l=\frac{\sum\_{i=1}^nQ\_i(z\_i=l)x\_i}{\sum\_{i=1}^nQ\_i(z\_i=l)}\)
   同理可以求得方差：</li>
</ul>

\[\Sigma\_j=\frac{\sum\_{i=1}^nQ\_i(z\_i=j)(x\_i-\mu\_j)(x\_i-\mu\_j)^T}{\sum\_{i=1}^nQ\_i(z\_i=j)}\]

<p>   现在来估计多项式分布\(\phi\)的分布：</p>

\[\sum\_{i=1}^n\sum\_{j=1}^KQ\_i(z\_i=j)log\frac{\frac{1}{(2\pi)^{n/2}|\Sigma\_j|^{1/2}}exp(-\frac{1}{2}(x\_i-\mu_i)^T\Sigma\_j^{-1}(x\_i-\mu_j))\phi\_j}{Q\_i(z\_i=j)}\]

<p>   因为咱们是要估计\(\phi\)，因此先删除无关的常数项，化简为：</p>

\[\sum\_{i=1}^n\sum\_{j=1}^KQ\_i(z\_i=j)log\phi\_j\]

<p>   概率的归一性，建立拉格朗日方程：</p>

\[L(\phi)=\sum\_{i=1}^n\sum\_{j=1}^KQ\_i(z\_i=j)log\phi\_j+\beta(\sum\_{j=1}^K\phi\_j-1)\]

<p>   有<code class="language-plaintext highlighter-rouge">log</code>函数在，求解到的\(\varphi_i\)一定是正的，因此不用考虑\(\varphi_i\)这个条件。
   求偏导，赋为0，得：
\(\frac{\partial}{\partial\phi\_j}L(\phi)=\sum\_{i}^n\frac{Q\_i(z\_i=j)}{\phi\_j}+\beta\)</p>

\[\sum\_{j=1}^K\phi\_j=1\]

\[\begin{align}
-\beta&amp;=\sum\_{i=1}^n\frac{Q\_i(z\_i=j)}{\phi\_j}\cdot1\\\\
&amp;=\sum\_{i=1}^n\frac{Q\_i(z\_i=j)}{\phi\_j}\cdot(\sum\_{j=1}^K\phi\_j)\\\\
&amp;=\sum\_{i=1}^n\sum\_{j=1}^KQ\_i(z\_i=j)\\\\
&amp;=\sum\_{i=1}^n1\\\\
&amp;=n
\end{align}\]

<p>   由此：</p>

\[\phi\_j=\frac{1}{n}\sum\_{i=1}^nQ\_i(z\_i=j)\]

<p>   这样就得到了结果：</p>

\[\begin{cases}
\mu\_l=\frac{\sum\_{i=1}^nQ\_i(z\_i=l)x\_i}{\sum\_{i=1}^nQ\_i(z\_i=l)}\\\\
\Sigma\_j=\frac{\sum\_{i=1}^nQ\_i(z\_i=j)(x\_i-\mu\_j)(x\_i-\mu\_j)^T}{\sum\_{i=1}^nQ\_i(z\_i=j)}\\\\
\phi\_j=\frac{1}{n}\sum\_{i=1}^nQ\_i(z\_i=j)
\end{cases}\]

<p>   直观的看我们用EM算法求到的结果和前面的直观解法得到的结果不太一样，但是他们其实是一样的。（仔细想想！！！）</p>

<h4 id="em算法的另一种理解">EM算法的另一种理解</h4>

<p>   其实EM算法可以看做坐标上升。所谓坐标上升就是每次通过更新函数中的一维，通过多次的迭代以达到优化函数的目的。</p>

\[J(Q,\theta)=\sum\_i\sum\_{z\_i}Q\_i(z\_i)log\frac{p(x\_i,z\_i;\theta)}{Q\_i(z\_i)}\]

<p>   上面的J就是前面咱们推到出的EM算法的目标函数。E-step就是在Q维上最大化J，M-step就是在\(\theta\)上最大化J。</p>

<h3 id="k-means和em算法的关系">k-means和EM算法的关系</h3>
<p>   可以发现EM算法的过程跟k-means算法的过程非常相似，都是先确定一个，更新另一个，再重新更新第一个，反复进行到收敛。实际上，k-means就是EM算法的一个实现。在k-means中先是假定隐含变量类别并将某一类别赋给每一个样例，这个过程就是E-step，M-step通过确定好的类别对其每个类的参数进行优化（这里就是更新类中心并重新分配类别）。</p>

<p>   总的来说，EM算法就是使用极大似然估计的方法，通过坐标上升的优化方法对隐变量和显变量的联合分布就行求最值的过程。</p>

<p>   好了，EM学完了，不得不说，EM算法中包含的东西真多啊！下一个准备学习的是—-&gt;<code class="language-plaintext highlighter-rouge">LDA</code>。</p>

<h2 id="参考链接">参考链接</h2>

<ul>
  <li><a href="http://www.cnblogs.com/rong86/p/3517573.html">k-means和em</a></li>
  <li><a href="http://blog.csdn.net/google19890102/article/details/51065297">坐标上升</a></li>
</ul>
